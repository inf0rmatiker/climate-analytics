{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2498ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-24T17:36:49.808776Z",
     "start_time": "2021-09-24T17:36:47.078490Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf. __version__) \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a3d43a",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For NOAA CSV Files\n",
    "\n",
    "# def get_features_and_target(csv_file):\n",
    "#     df = pd.read_csv(f'../spark_output/colorado/{csv_file}')\n",
    "#     df.drop(['year_month_day'], axis=1, inplace=True)\n",
    "\n",
    "#     features = df.iloc[:, 1:-1].values\n",
    "#     target = df.iloc[:, 0].values.reshape(-1,1)\n",
    "\n",
    "#     assert features.shape[0] == target.shape[0]\n",
    "#     assert target.shape[1] == 1\n",
    "\n",
    "#     return features.astype('float64'), target.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a2db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_target(csv_file):\n",
    "    df = pd.read_csv(f'../datasets/csu-weather-data/colorado/2020-2021/{csv_file}')\n",
    "    df.drop(['date'], axis=1, inplace=True)\n",
    "    columns = ['max_temperature', 'min_temperature', 'precipitation', 'snowfall']\n",
    "\n",
    "    features = df[['max_temperature', 'min_temperature', 'precipitation']].values\n",
    "    target = df['snowfall'].values.reshape(-1, 1)\n",
    "    target\n",
    "\n",
    "    assert features.shape[0] == target.shape[0]\n",
    "    assert target.shape[1] == 1\n",
    "\n",
    "    return features.astype('float64'), target.astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78dd08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(X, T):\n",
    "    \n",
    "    \n",
    "    # Calculate standardization parameters \n",
    "    x_means = np.mean(X, axis=0)\n",
    "    x_stds = np.std(X , axis=0)\n",
    "    x_stds[x_stds == 0] = 1\n",
    "    \n",
    "    t_means = np.mean(T, axis=0)\n",
    "    t_stds = np.std(T, axis=0)\n",
    "    t_stds[t_stds == 0] = 1\n",
    "    \n",
    "    X = (X - x_means) / x_stds\n",
    "    T = (T - t_means) / t_stds\n",
    "    \n",
    "    x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.20, random_state=42)\n",
    "\n",
    "    # 0.25 x 0.8 = 0.2\n",
    "    x_train, x_validate, t_train, t_validate = train_test_split(x_train, t_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    assert x_train.shape[0] == t_train.shape[0]\n",
    "    assert x_test.shape[0] == t_test.shape[0]\n",
    "    assert x_validate.shape[0] == t_validate.shape[0]\n",
    "\n",
    "    return [x_train, x_test, x_validate], [t_train, t_test, t_validate]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1ccbe",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a745e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(3, )))\n",
    "    \n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units = hp.Int('units', min_value=2, max_value=50, step=2)\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4, 5e-2, 5e-3, 5e-4])\n",
    "    \n",
    "              \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss='mae',\n",
    "                metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e67563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "def run_model_search(county):\n",
    " \n",
    "    features, target = get_features_and_target(county)\n",
    "    X, T = split_train_test_val(features, target)\n",
    "  \n",
    "    x_train, x_test, x_validate = X\n",
    "    t_train, t_test, t_validate = T\n",
    "    \n",
    "    \n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder,\n",
    "        objective='loss',\n",
    "        max_epochs=10,\n",
    "        factor=3,\n",
    "        directory='SnowPredictions',\n",
    "        project_name=county)\n",
    "    \n",
    "    \n",
    "    stop_early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    tuner.search(x_train, t_train, epochs=200, validation_data=(x_validate, t_validate), callbacks=[stop_early])\n",
    "    \n",
    "    # Get the optimal hyperparameters\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=100)[0]\n",
    "    \n",
    "    \n",
    "    # Build the model with the optimal hyperparameters\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    history = model.fit(x_train, t_train, epochs=200, validation_data=(x_validate, t_validate))\n",
    "    \n",
    "    mae_per_epoch = history.history['mae']\n",
    "    best_epoch = mae_per_epoch.index(max(mae_per_epoch)) + 1\n",
    "    print('Best epoch: %d' % (best_epoch,))\n",
    "    \n",
    "    hypermodel = tuner.hypermodel.build(best_hps)\n",
    "    \n",
    "    # Retrain the model\n",
    "    history = hypermodel.fit(x_train, t_train, epochs=200, validation_data=(x_validate, t_validate))\n",
    "    history.history['county'] = county\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    eval_result = hypermodel.evaluate(x_test, t_test)\n",
    "    print(\"[test loss, test mae]:\", eval_result)\n",
    "    \n",
    "    return hypermodel, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1b2d3",
   "metadata": {},
   "source": [
    "## Graph Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de144264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_loss(model_history):\n",
    "\n",
    "    history = model_history.history\n",
    "    \n",
    "    county = history['county']\n",
    "    county = county[:county.index('.')]\n",
    "    train_loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.plot(train_loss, '-b', label='Training loss')\n",
    "    plt.plot(val_loss, '-g', label='Validation loss')\n",
    "    plt.title(f\"{county}'s Training and Validation Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'LossGraphs/{county}', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df55dc8",
   "metadata": {},
   "source": [
    "## Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000036e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "counties_path = '../datasets/csu-weather-data/colorado/2020-2021/'\n",
    "colorado_counties = [f for f in listdir(counties_path) if isfile(join(counties_path, f))]\n",
    "colorado_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e4f61",
   "metadata": {},
   "source": [
    "## All Models and History For All Counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f593b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colorado_county_models = []\n",
    "colorado_county_model_histories = []\n",
    "\n",
    "for county in colorado_counties:\n",
    "    model, history = run_model_search(county)\n",
    "    graph_loss(history)\n",
    "    colorado_county_models.append(model)\n",
    "    colorado_county_model_histories.append(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2385303",
   "metadata": {},
   "source": [
    "<br />\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a125d06e",
   "metadata": {},
   "source": [
    "## For NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gunnison = \"Gunnison.csv\"\n",
    "# fremont = \"Fremont.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e64145",
   "metadata": {},
   "source": [
    "### Gunnison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a1fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gunnison_model, gunnison_history = run_model_search(gunnison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196396f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display_loss([gunnison_history])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd06bf",
   "metadata": {},
   "source": [
    "### Fremont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd627b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fremont_model, fremont_history = run_model_search(fremont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_loss([fremont_history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8ba99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f8cb2c04e123a5f36026e83ef04ba8d8cb2e5b8983b92be00cf5b9b6813ba1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
